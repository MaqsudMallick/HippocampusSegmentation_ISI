{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations==0.4.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i10LCLUD1FbV",
        "outputId": "32b75010-0d70-4927-c64d-4f810eedabfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: albumentations==0.4.6 in /usr/local/lib/python3.7/dist-packages (0.4.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.21.6)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.8.2)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.3.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.4.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1KfCOB41HAH",
        "outputId": "305df615-ee53-4e25-e225-35576aacede4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpobOzat7mP5"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import sklearn\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "from torchinfo import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fFzUlXWg-Qo"
      },
      "outputs": [],
      "source": [
        "class HippocampusDataset(Dataset):\n",
        "  def __init__(self, image_dir, mask_dir, transform=None):\n",
        "    self.image_dir = image_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "    self.images = os.listdir(image_dir)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img_path = os.path.join(self.image_dir, self.images[index])\n",
        "    mask_path = os.path.join(self.mask_dir, self.images[index])\n",
        "    image = np.array(Image.open(img_path).convert(\"RGB\"),dtype=np.float32)\n",
        "    # print((h,w,c))\n",
        "    # image = np.array([[[[image[i][j][k] for j in range(0,w)] for i in range (0,h)] for k in range(0,c)]])\n",
        "    mask = np.array(Image.open(mask_path),dtype=np.float32)\n",
        "    # mask[mask==255.0]=1.0\n",
        "    # print(mask.shape)\n",
        "\n",
        "    if self.transform is not None:\n",
        "      augmentations = self.transform(image=image, mask=mask)\n",
        "      image=augmentations[\"image\"]\n",
        "      mask=augmentations[\"mask\"]\n",
        "    # h,w,c = np.array(image).shape\n",
        "    # print(h,w,c)\n",
        "    # image = np.array([[[[image[i][j][k] for j in range(0,w)] for i in range (0,h)] for k in range(0,c)]])\n",
        "    # mask=np.array(mask.convert('L'),dtype=np.float32)\n",
        "    # print(np.unique(mask),mask.shape)\n",
        "\n",
        "    return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4EuNhFVmxfz"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "lr = 1e-4\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "batch_size = 8\n",
        "epochs = 5\n",
        "img_height = 512\n",
        "img_width = 512\n",
        "train_img_dir = \"./train\"\n",
        "train_mask_dir = \"./train_mask\"\n",
        "val_img_dir = \"./val\"\n",
        "val_mask_dir = \"./val_mask\"\n",
        "load_model = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoL9woDC9zPJ"
      },
      "outputs": [],
      "source": [
        "class conv_block(nn.Module):\n",
        "  def __init__(self, inp, out):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(inp, out, kernel_size=3, padding=1).to(device)\n",
        "    self.bn1 = nn.BatchNorm2d(out).to(device)\n",
        "    self.conv2 = nn.Conv2d(out, out, kernel_size=3, padding=1).to(device)\n",
        "    self.bn2 = nn.BatchNorm2d(out).to(device)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, input):\n",
        "    x = self.conv1(input)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.relu(x)\n",
        "    return x\n",
        "\n",
        "class encoder(nn.Module):\n",
        "  def __init__(self,inp,out):\n",
        "    super().__init__()\n",
        "    self.conv = conv_block(inp,out).to(device)\n",
        "    self.pool = nn.MaxPool2d(2,2).to(device)\n",
        "\n",
        "  def forward(self,inp):\n",
        "    x = self.conv(inp)\n",
        "    p = self.pool(x)\n",
        "    return [x,p]\n",
        "\n",
        "class decoder(nn.Module):\n",
        "  def __init__(self,inp,out,size):\n",
        "    super().__init__()\n",
        "    self.up = nn.ConvTranspose2d(inp,out,kernel_size=2,stride=2,padding=0).to(device)\n",
        "    self.conv = conv_block(size,out).to(device)\n",
        "\n",
        "  def forward(self,inp,skip):\n",
        "    x = self.up(inp)\n",
        "    skip.insert(0,x)\n",
        "    x = torch.cat(skip, axis=1)\n",
        "    x = self.conv(x)\n",
        "    return x\n",
        "\n",
        "class unetplusplus(nn.Module):\n",
        "  def __init__(self,l):\n",
        "    super().__init__()\n",
        "    n = len(l)-3\n",
        "    self.e = []\n",
        "    self.d = []\n",
        "    for i in range(0,n):\n",
        "      self.e.append(encoder(l[i],l[i+1]))\n",
        "      self.d.append([])\n",
        "      for j in range(0,n-i):\n",
        "        self.d[i].append(decoder(l[i+2],l[i+1],(j+2)*l[i+1]))\n",
        "    self.b = conv_block(l[n],l[n+1]).to(device)\n",
        "    self.out = nn.Conv2d(l[1],l[n+2],kernel_size=1,padding=0).to(device)\n",
        "\n",
        "  def forward(self,inp):\n",
        "    n = len(self.e)\n",
        "    y = []\n",
        "    for i in range(0,n):\n",
        "      y.append([self.e[i](inp if i==0 else y[i-1][0][1])])\n",
        "    y.append([self.b(y[n-1][0][1])])\n",
        "    for i in range(0,n-1):\n",
        "      y[i].append(self.d[i][0](y[i+1][0][0],[y[i][0][0]]))\n",
        "    y[n-1].append(self.d[n-1][0](y[n][0],[y[n-1][0][0]]))\n",
        "    for j in range(1,n+1):\n",
        "      for i in range(1,n+1-j):\n",
        "        y[i-1].append(self.d[i-1][j-1](y[i][j],[y[i-1][k][0] if k==0 else y[i-1][k] for k in range(0,j)]))\n",
        "    out = self.out(y[0][n])\n",
        "    return out\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#   h = 256\n",
        "#   w = 512\n",
        "#   c = 2\n",
        "#   input = torch.randn((1,3,h,w)).to(device)\n",
        "#   print(input.shape)\n",
        "#   model = unetplusplus([3,64,128,512,1])\n",
        "#   y = model(input)\n",
        "#   print(y.shape)\n",
        "#   #summary(model, input_size=(3, 52, 36))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpKfnL1HRXqn",
        "outputId": "9f2dbd57-1616-4d51-a338-38cdf06476ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "unetplusplus                             [16, 1, 36, 52]           --\n",
              "├─conv_block: 1-1                        [16, 256, 9, 13]          --\n",
              "│    └─Conv2d: 2-1                       [16, 256, 9, 13]          295,168\n",
              "│    └─BatchNorm2d: 2-2                  [16, 256, 9, 13]          512\n",
              "│    └─ReLU: 2-3                         [16, 256, 9, 13]          --\n",
              "│    └─Conv2d: 2-4                       [16, 256, 9, 13]          590,080\n",
              "│    └─BatchNorm2d: 2-5                  [16, 256, 9, 13]          512\n",
              "│    └─ReLU: 2-6                         [16, 256, 9, 13]          --\n",
              "├─Conv2d: 1-2                            [16, 1, 36, 52]           65\n",
              "==========================================================================================\n",
              "Total params: 886,337\n",
              "Trainable params: 886,337\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 1.66\n",
              "==========================================================================================\n",
              "Input size (MB): 0.36\n",
              "Forward/backward pass size (MB): 15.58\n",
              "Params size (MB): 3.55\n",
              "Estimated Total Size (MB): 19.48\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input = torch.randn((16,3,36,52))\n",
        "input = input.to(device)\n",
        "model_test = unetplusplus([3,64,128,256,1])\n",
        "model_test = model_test.to(device)\n",
        "from torchinfo import summary\n",
        "summary(model_test,input_data=input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GF9d2NWjZGYn"
      },
      "outputs": [],
      "source": [
        "def get_loaders(train_img_dir,\n",
        "      train_mask_dir,\n",
        "      val_img_dir,\n",
        "      val_mask_dir,\n",
        "      batch_size,\n",
        "      train_transform,\n",
        "      val_transform):\n",
        "  train_dataset = HippocampusDataset(\n",
        "      image_dir=train_img_dir,\n",
        "      mask_dir=train_mask_dir,\n",
        "      transform=train_transform,\n",
        "  )\n",
        "  # print(train_dataset)\n",
        "  train_loader = DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "  )\n",
        "\n",
        "  validation_dataset = HippocampusDataset(\n",
        "      image_dir=val_img_dir,\n",
        "      mask_dir=val_mask_dir,\n",
        "      transform=val_transform,\n",
        "  )\n",
        "\n",
        "  validation_loader = DataLoader(\n",
        "      validation_dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "  )\n",
        "\n",
        "  return train_loader, validation_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LB7oEFf4ThaW"
      },
      "outputs": [],
      "source": [
        "def check_accuracy(loader, model, device=\"cuda\"):\n",
        "  print(\"accuracy\")\n",
        "  num_correct = 0\n",
        "  num_pixels = 0\n",
        "  dice_score = 0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y in loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device).unsqueeze(1)\n",
        "      preds = torch.sigmoid(model(x))\n",
        "      preds = (preds>0.5).float()\n",
        "      num_correct += (preds==y).sum()\n",
        "      num_pixels += torch.numel(preds)\n",
        "      dice_score += (2*(preds*y).sum())/((preds+y).sum() + 1e-8)\n",
        "\n",
        "  print(f\"{num_correct}/{num_pixels} with accuracy {num_correct/num_pixels*100:.2f}\")\n",
        "  print(f\"Dice Score: {dice_score/len(loader)}\")\n",
        "\n",
        "  model.train()\n",
        "\n",
        "\n",
        "def save_predicted_images(loader, model, folder=\"saved_images/\", device=device):\n",
        "  model.eval()\n",
        "  for idx, (x,y) in enumerate(loader):\n",
        "    x = x.to(device)\n",
        "    with torch.no_grad():\n",
        "      preds = torch.sigmoid(model(x))\n",
        "      # preds = (preds>0.5).float()\n",
        "      preds *= 255.0\n",
        "    torchvision.utils.save_image(preds, f\"{folder}/pred_{idx}.png\")\n",
        "    torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}/actual_{idx}.png\")\n",
        "\n",
        "  model.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odbE7E40TXUP"
      },
      "outputs": [],
      "source": [
        "def train(loader, model, optimizer, loss_fn, scaler):\n",
        "  loop = tqdm(loader)\n",
        "  for batch_idx, (data,targets) in enumerate(loop):\n",
        "    data = data.to(device=device)\n",
        "    # data = np.array([[[[data[i][j][k] for j in range(0,w)] for i in range (0,h)] for k in range(0,c)]])\n",
        "    targets = targets.float().unsqueeze(1).to(device=device)\n",
        "    # print(targets.shape)\n",
        "    # print(np.unique(targets.cpu()))\n",
        "\n",
        "    #forward\n",
        "    with torch.cuda.amp.autocast():\n",
        "      # print(data.shape)\n",
        "      predictions = model(data)\n",
        "      # print(predictions.shape)\n",
        "      # img_mask_predictions = predictions.detach().numpy()[0]\n",
        "      # print(img_mask_predictions.shape)\n",
        "      # print(targets.shape)\n",
        "      # img_mask_predictions = np.array([[img_mask_predictions[0][i][j] for j in range(0,w)] for i in range(0,h)])\n",
        "      loss = loss_fn(0.5+0.5*predictions,targets)\n",
        "\n",
        "    #backward\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    #update tqdm loop\n",
        "    loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    #return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWDigcW0Rlm8"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, filename=\"unetplusplus_checkpoint.tar\"):\n",
        "  print(\"Saving checkpoint --->\")\n",
        "  torch.save(state,filename)\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "  print(\"Loading checkpoint --->\")\n",
        "  model.load_state_dict(checkpoint[\"state_dict\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "ufl3h0UJUirn",
        "outputId": "e4d4e2b3-3ae8-4f53-84cf-ee757c6803fb",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch ---> 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|                                                                                         | 0/1766 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 4.00 GiB total capacity; 3.28 GiB already allocated; 0 bytes free; 3.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 74>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m     save_predicted_images(val_loader, model, folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./saveimg/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 75\u001b[0m   \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;66;03m#train\u001b[39;00m\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch --->\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m   \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m   \u001b[38;5;66;03m#save checkpoint\u001b[39;00m\n\u001b[0;32m     61\u001b[0m   checkpoint \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     62\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m     63\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m     64\u001b[0m   }\n",
            "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(loader, model, optimizer, loss_fn, scaler)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# print(targets.shape)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# print(np.unique(targets.cpu()))\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#forward\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m     12\u001b[0m   \u001b[38;5;66;03m# print(data.shape)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m   predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m   \u001b[38;5;66;03m# print(predictions.shape)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m   \u001b[38;5;66;03m# img_mask_predictions = predictions.detach().numpy()[0]\u001b[39;00m\n\u001b[0;32m     16\u001b[0m   \u001b[38;5;66;03m# print(img_mask_predictions.shape)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m   \u001b[38;5;66;03m# print(targets.shape)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m   \u001b[38;5;66;03m# img_mask_predictions = np.array([[img_mask_predictions[0][i][j] for j in range(0,w)] for i in range(0,h)])\u001b[39;00m\n\u001b[0;32m     19\u001b[0m   loss \u001b[38;5;241m=\u001b[39m loss_fn(\u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39mpredictions,targets)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36munetplusplus.forward\u001b[1;34m(self, inp)\u001b[0m\n\u001b[0;32m     62\u001b[0m y\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb(y[n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m])])\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 64\u001b[0m   y[i]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     65\u001b[0m y[n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md[n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m](y[n][\u001b[38;5;241m0\u001b[39m],[y[n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]]))\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,n\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mdecoder.forward\u001b[1;34m(self, inp, skip)\u001b[0m\n\u001b[0;32m     38\u001b[0m skip\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m,x)\n\u001b[0;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(skip, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mconv_block.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m     11\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     14\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2438\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2436\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2439\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2440\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 4.00 GiB total capacity; 3.28 GiB already allocated; 0 bytes free; 3.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "model = unetplusplus([3,64,128,256,1]).to(device)\n",
        "def main():\n",
        "  train_transform = A.Compose(\n",
        "      [\n",
        "            # A.Resize(height=img_height,width=img_width),\n",
        "            A.Rotate(limit= 10, p=0.2),\n",
        "#             A.Sharpen(),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.5),\n",
        "            A.Normalize(\n",
        "                mean=[0.0,0.0,0.0],\n",
        "                std=[1.0,1.0,1.0],\n",
        "                max_pixel_value=255.0,\n",
        "            ),\n",
        "            ToTensorV2()\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  val_transform = A.Compose(\n",
        "      [\n",
        "            # A.Resize(height=img_height,width=img_width),\n",
        "            # A.Rotate(limit=10),\n",
        "            # A.HorizontalFlip(p=0.5),\n",
        "            # A.VerticalFLip(p=0.5),\n",
        "            A.Normalize(\n",
        "                mean=[0.0,0.0,0.0],\n",
        "                std=[1.0,1.0,1.0],\n",
        "                max_pixel_value=255.0,\n",
        "            ),\n",
        "            ToTensorV2()\n",
        "      ]\n",
        "  )\n",
        "\n",
        "#   model = unetplusplus([3,128,256,512,1]).to(device)\n",
        "  loss_fn = nn.BCEWithLogitsLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "  scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "  train_loader, val_loader = get_loaders(\n",
        "      train_img_dir,\n",
        "      train_mask_dir,\n",
        "      val_img_dir,\n",
        "      val_mask_dir,\n",
        "      batch_size,\n",
        "      train_transform,\n",
        "      val_transform,\n",
        "  )\n",
        "  # for batch_idx, (data,targets) in enumerate(train_loader):\n",
        "  #   print(data.shape)\n",
        "  #   print(targets.shape)\n",
        "\n",
        "  if load_model:\n",
        "    load_checkpoint(torch.load(\"./unetplusplus_checkpoint.tar\"), model)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    #train\n",
        "    print(\"Epoch --->\", epoch+1)\n",
        "    train(train_loader, model, optimizer, loss_fn, scaler)\n",
        "\n",
        "    #save checkpoint\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    save_checkpoint(checkpoint)\n",
        "\n",
        "    #check accuracy\n",
        "    check_accuracy(val_loader, model, device=device)\n",
        "\n",
        "    #save images\n",
        "    save_predicted_images(val_loader, model, folder=\"./saveimg/\")\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64GlH8Nm0ynS",
        "outputId": "8a3521ef-6bfb-4e92-ad28-7068df5a8841"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 3, 36, 52)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "image = cv2.imread('./sample.png')\n",
        "image = cv2.resize(image, (52, 36))\n",
        "h, w, c = np.array(image).shape\n",
        "arr = np.array([ [[[image[i][j][k] for j in range(0,w)] for i in range (0,h)] for k in range(0,c)] ])\n",
        "print(arr.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emzuO2dI0ynS"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "out = model(torch.tensor((arr.astype(np.float32))/255.0).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGfiVgH20ynT",
        "outputId": "9f47d078-97d8-40e7-91e4-c2412451dd55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-8.96107337e+17 -1.50893801e+18 -1.39053806e+18 ... -1.05260069e+18\n",
            "  -1.44394175e+18 -1.15123657e+18]\n",
            " [-1.19142173e+18 -1.26354282e+18 -9.48723916e+17 ... -8.16557121e+17\n",
            "  -1.45416460e+18 -8.90311192e+17]\n",
            " [-1.48984183e+18 -1.54249785e+18 -1.23488446e+18 ... -9.18387153e+17\n",
            "  -1.23593257e+18 -1.04108021e+18]\n",
            " ...\n",
            " [-1.22003583e+18 -1.57700740e+18 -1.25460956e+18 ... -1.43514030e+18\n",
            "  -1.46538292e+18 -1.40554612e+18]\n",
            " [-1.71726220e+18 -8.15413491e+17 -1.16914975e+18 ... -1.27446166e+18\n",
            "  -1.13188015e+18 -1.03767131e+18]\n",
            " [-1.08604124e+18 -1.09595464e+18 -1.19795778e+18 ... -8.71117774e+17\n",
            "  -1.10950894e+18 -9.08130222e+17]]\n",
            "[[-4.60172657e+17 -1.07300336e+18 -9.54603417e+17 ... -6.16666044e+17\n",
            "  -1.00800711e+18 -7.15301858e+17]\n",
            " [-7.55487084e+17 -8.27608175e+17 -5.12789237e+17 ... -3.80622441e+17\n",
            "  -1.01822996e+18 -4.54376513e+17]\n",
            " [-1.05390718e+18 -1.10656321e+18 -7.98949816e+17 ... -4.82452474e+17\n",
            "  -7.99997926e+17 -6.05145499e+17]\n",
            " ...\n",
            " [-7.84101187e+17 -1.14107275e+18 -8.18674918e+17 ... -9.99205656e+17\n",
            "  -1.02944827e+18 -9.69611476e+17]\n",
            " [-1.28132756e+18 -3.79478812e+17 -7.33215101e+17 ... -8.38527012e+17\n",
            "  -6.95945506e+17 -6.01736600e+17]\n",
            " [-6.50106591e+17 -6.60019925e+17 -7.62023131e+17 ... -4.35183095e+17\n",
            "  -6.73574292e+17 -4.72195542e+17]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x171f34c7700>"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 480x320 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "out = out*255\n",
        "out = out.to('cpu')\n",
        "img_mask = out.detach().numpy()[0]\n",
        "a = np.array([[img_mask[0][i][j] for j in range(0,w)] for i in range(0,h)])\n",
        "print(a)\n",
        "a = np.array([[np.max(a)+j if j<0 else j for j in i] for i in a])\n",
        "print(a)\n",
        "plt.figure(dpi=80)\n",
        "plt.imshow(a, cmap= 'gray')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "4be3aec549d3f104a0bccb5f9f06424c94da1818920503d30f09042a83a0516e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}